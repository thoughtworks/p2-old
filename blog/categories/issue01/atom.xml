<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: issue01 | P2 Magazine]]></title>
  <link href="http://thoughtworks.github.io/p2/blog/categories/issue01/atom.xml" rel="self"/>
  <link href="http://thoughtworks.github.io/p2/"/>
  <updated>2014-02-04T17:51:46-05:00</updated>
  <id>http://thoughtworks.github.io/p2/</id>
  <author>
    <name><![CDATA[The P2 Elves]]></name>
    <email><![CDATA[p2@thoughtworks.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Credits]]></title>
    <link href="http://thoughtworks.github.io/p2/issue01/credits/"/>
    <updated>2013-06-07T00:00:00-04:00</updated>
    <id>http://thoughtworks.github.io/p2/issue01/credits</id>
    <content type="html"><![CDATA[<p><strong>Editorial Committee</strong></p>

<p>Ryan Boucher, Mike Gardiner, Rachel Laycock, Rebecca Parsons, Nick Thorpe</p>

<p><strong>Writers</strong></p>

<p>Ryan Boucher, Mike Gardiner, Jen Smith</p>

<p><strong>Art</strong></p>

<p>Greg Skinner</p>

<p><strong>Site Design</strong></p>

<p>Andrew Carr, Mike Gardiner</p>

<p><strong>Photo Credit</strong></p>

<p>The cover photo is of Yangshuo in Guangxi province taken by Adam Scott during his twelve month transfer to ThoughtWorks China</p>

<p><strong>Special Thanks</strong></p>

<p>Anand Bagmar, Dave Coombes</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Puzzle]]></title>
    <link href="http://thoughtworks.github.io/p2/issue01/puzzle/"/>
    <updated>2013-06-06T00:00:00-04:00</updated>
    <id>http://thoughtworks.github.io/p2/issue01/puzzle</id>
    <content type="html"><![CDATA[<p>This month&rsquo;s coding challenge visits us from the land of Java. What we have is a question of execution order. The first person to respond with the correct answer and why gets their name in the winners circle for next issue.</p>

<p>Exhibit A &mdash; What is the result of this function?</p>

<div class='normal-gist'><script src='https://gist.github.com/distributedlife/5692366.js'></script></div>


<p>Exhibit B &mdash; What is the value of &lsquo;value&rsquo; after this function is executed?</p>

<div class='normal-gist'><script src='https://gist.github.com/distributedlife/5692369.js'></script></div>


<p>Send your answers to <a href=\"mailto:p2@thoughtworks.com\"><a href="&#109;&#97;&#x69;&#108;&#116;&#111;&#x3a;&#112;&#50;&#x40;&#x74;&#x68;&#x6f;&#x75;&#x67;&#x68;&#116;&#x77;&#111;&#x72;&#x6b;&#115;&#46;&#99;&#x6f;&#109;">&#112;&#50;&#x40;&#x74;&#104;&#x6f;&#117;&#103;&#x68;&#116;&#119;&#111;&#114;&#107;&#x73;&#x2e;&#99;&#111;&#109;</a></a></p>

<div class='byline'>All Gists brought to you by <a href='http://github.com/'>GitHub</a></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improve This]]></title>
    <link href="http://thoughtworks.github.io/p2/issue01/improve-this/"/>
    <updated>2013-06-05T00:00:00-04:00</updated>
    <id>http://thoughtworks.github.io/p2/issue01/improve-this</id>
    <content type="html"><![CDATA[<p>In improve this we take a look at a reader submitted test, user interface, story or block of code and we try and improve it, without context, explaining what we did as we went.</p>

<p>In this issue Anand has sent us a cucumber test that he’s asked us to improve.</p>

<script src='https://gist.github.com/distributedlife/5692407.js'></script>


<p>First up some incidental detail. “Given I or anyone else is, has or will log in” is the most ubiquitous waste of time that exists in the land of cucumber. Unless you&rsquo;re testing user authentication, delete it.</p>

<div class='normal-gist'><script src='https://gist.github.com/distributedlife/5692415.js'></script></div>


<p>The usernames and passwords are not required here. The roles are potentially useful but having buyer_1 and buyer_2 as unique values for a ‘role’ – that represents a classification – is a smell. Who they ‘bid on behalf of’ for is never mentioned again so that is also incidental detail.</p>

<script src='https://gist.github.com/distributedlife/5692421.js'></script>


<p>‘Created’ can be implied from the fact you are publishing it and as the automatic run assignment is never mentioned so it might be incidental too. I’m inclined to delete it.</p>

<p>Next.</p>

<script src='https://gist.github.com/distributedlife/5692423.js'></script>


<p>Ok. Although I would ignore how they get added to the Live Sale. And as listings seem to be of little consequence through the rest of the script I would delete this line entirely.</p>

<p>Next.</p>

<script src='https://gist.github.com/distributedlife/5692452.js'></script>


<p>The &lsquo;before Preview Start Time&rsquo; refers to a state that a sale exists in. Implying state is bad because you end up referring to it in roundabout ways. Make it a first class part of your system. I&rsquo;m not sure what this state is but it might be something like preparation. We also need to consider whether anyone “cannot” suspend the live sale or if only the admin cannot suspend the live sale. I suspect it’s anyone cannot suspend it. So let’s remove the admin reference.</p>

<script src='https://gist.github.com/distributedlife/5692457.js'></script>


<p>Next.</p>

<script src='https://gist.github.com/distributedlife/5692489.js'></script>


<p>Ok, now we can suspend the auction as it has started. So we should make this a separate test rather than a flow. Here we have an admin suspending an auction. Do we need tests for other roles not having the ability to suspend auctions?</p>

<script src='https://gist.github.com/distributedlife/5692492.js'></script>


<p>Next.</p>

<div class='normal-gist'><script src='https://gist.github.com/distributedlife/5692495.js'></script></div>


<p>Ok, so now we have some behavior where logged-in users are shown a message when a sale is suspended. It&rsquo;s not clear why buyer_2 does not see get notified. I can&rsquo;t see any buyer_1 steps earlier on, that don&rsquo;t include buyer_2.</p>

<p>I don&rsquo;t like implementation detail in tests. This test implies that only users online now, will be notified of the Live Sale suspension.</p>

<script src='https://gist.github.com/distributedlife/5692506.js'></script>


<p>We test that we notify users of the suspension but not how they are notified.</p>

<p>Next.</p>

<div class='normal-gist'><script src='https://gist.github.com/distributedlife/5692513.js'></script></div>


<p>Here we are testing for the absence of something. Do any users see the popup? If not, how can we test for something that was never built. Deleted.</p>

<p>Next.</p>

<script src='https://gist.github.com/distributedlife/5692519.js'></script>


<p>The auction starts with an initial bid. However it&rsquo;s not clear if we are testing that only an auctioneer can set the starting bid or, if setting the start bid starts the auction. I’m inclined to say the latter.</p>

<script src='https://gist.github.com/distributedlife/5692521.js'></script>


<p>Next.</p>

<script src='https://gist.github.com/distributedlife/5692524.js'></script>


<p>Ok, buyer 1 and 2 have both bid now. Then the auctioneer does a Sell and then a No Sale on the listing. What is a listing? It has only been mentioned once before on the second line of the script.</p>

<p>The last verification suggests that sales can be suspended but only if it is in the No Sale state. However there is no guarantee of this. I’m not sure what ‘No Sale’ means exactly. It’s a strange name for a state but i’ll let it pass but would no called it a ‘No Sale auction’ but a ‘No Sale’.</p>

<script src='https://gist.github.com/distributedlife/5692528.js'></script>


<p>Next.</p>

<script src='https://gist.github.com/distributedlife/5692539.js'></script>


<p>Listings are back and they can run. The important thing here is that the auctioneer ends the auction after the listings have run. Not that having the listings finish running automatically ends the auction. I see listings as not important.</p>

<script src='https://gist.github.com/distributedlife/5692540.js'></script>


<p>Done.</p>

<p>The main things we covered were around domain language and removing the implementation from the notifications. We removed lots of incidental detail and clarified some behaviour around state transition and who is authorised to do what.</p>

<p>We’ve made a lot of assumptions about how this system works and how the cucumber file is used. We’ve ignored how the cucumber file got to where it is today and followed a simple rule: if someone can’t understand how it works from reading it, then it needs improvement.</p>

<p>Do you have something you want improved? Send it to <a href='mailto:p2@thoughtworks.com'><a href="&#x6d;&#97;&#x69;&#108;&#116;&#111;&#x3a;&#112;&#50;&#x40;&#x74;&#x68;&#111;&#117;&#103;&#x68;&#116;&#x77;&#x6f;&#114;&#x6b;&#x73;&#x2e;&#x63;&#x6f;&#x6d;">&#x70;&#50;&#x40;&#x74;&#x68;&#111;&#x75;&#103;&#104;&#x74;&#119;&#x6f;&#114;&#x6b;&#x73;&#x2e;&#99;&#x6f;&#x6d;</a></a>.</p>

<div class='byline'>All Gists brought to you by <a href='http://github.com/'>GitHub</a></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The King Is Dead, Long Live The King]]></title>
    <link href="http://thoughtworks.github.io/p2/issue01/the-king-is-dead/"/>
    <updated>2013-06-04T00:00:00-04:00</updated>
    <id>http://thoughtworks.github.io/p2/issue01/the-king-is-dead</id>
    <content type="html"><![CDATA[<p>I stood in front of the board, asking unpopular questions. &ldquo;When was the last time we stopped offering a service that was losing money? Do we even know which ones make money?”   I could see confusion and anger on the faces of the board members. It&rsquo;s not a pleasant feeling standing in a hostile room telling people that they should stop offering services that now, because of me, no longer exist.</p>

<p>My name is Monika and to understand what I was talking about and how I ended up in the boardroom I have to go back 2 years to the very start of project Reboot. The project was an 84-million dollar effort to rewrite our core systems. The original system had a lots of bugs in it and there were no automated tests. We had over 300 systems covering generations of technology. Like ancient rock, we had layers: Mainframes, Visual Basic, .NET 1, 2, 3 and 4 – all integrated through a tangle of service buses and queues.</p>

<p>Within a month we had serious problems. Each release into test resulted in regressions. We&rsquo;d spend more time retesting than we would testing. We were going backwards. I was not sleeping.</p>

<p>We knew we had to bring in test automation. But the environment was too brittle. We&rsquo;d get a new build on Thursday and the automation would break. The we&rsquo;d waste time getting it fixed before we could start testing any of the new work. The problem, it turned out, was twofold. Our tests were reliant on data that didn&rsquo;t always exist, and of course the neighbouring systems also had their own bugs.</p>

<p>My days were spent reacting to the latest testing problem. My nights were spent reading books and blogs trying to solve the bigger problem.</p>

<p>I was ragged. I met my friend Maggie for a coffee.</p>

<p>&ldquo;Mon, you need to sleep. You spend your nights trying to solve these problems. Hire a consultant to do that during the day so you can sleep.” Maggie was right. I just needed some of her decisiveness and a new perspective.</p>

<p>The consultant told me the tests would be fixed by creating golden test data and by building a fake versions of the neighbouring systems. As consultants do, she solved one problem by slicing out lots of little problems. Golden data took time to curate and each fake service could only serve a handful of tests.</p>

<p>This worked well until we had hundreds of these micro fake services. They were growing like a garden of weeds. We couldn&rsquo;t keep this up. \&ldquo;I don&rsquo;t want any more single-use fakes.\&rdquo; I yelled across the development floor.</p>

<p>My team devised a façade that would proxy the messages to the correct micro fake. This collapsed our smorgasbord of fakes down to one per system. We wanted to do things properly so we wrote a fake consumer to test our façade.</p>

<p>Things slowly started to improve and I even got a few weekends to myself. The tests were robust and they found bugs – lots of them. The issue I had now was the rework required to fix the bugs. What a waste! We were still behind schedule, but we were maintaining pace. I wanted to – needed to – claw back the lost month. To reduce rework I asked the developers to write the tests first.</p>

<p>They refused.</p>

<p>&ldquo;It&rsquo;s too hard. All the fakes we&rsquo;ve written only work in test.” And on and on. The fake services relied on the golden dataset, which at a whopping 40 gigabytes was too big to put onto every developer&rsquo;s workstation.  They also needed to behave differently depending on which environment they were in. In development they needed to be simpler, in test, fakes talk to other fakes.</p>

<p>I refused to be refused. But I had to find a solution. We started making the fake services environmentally aware. It was tricky but it worked. Fixing the data problem was more effort. We wrote fakes that would do the work of creating the data for us. This allowed us to get rid of the golden data. It was so successful that we almost missed a bug when one of the developers used a fake for creating data rather than using the more cumbersome ‘real' services.</p>

<p>Now that developers were also writing tests, they were missing opportunities for sharing test logic because they didn&rsquo;t always know what existing tests did. Each tests worked out which fakes to use and which validation to run. Often the problem had already been solved elsewhere, leading to massive duplication.</p>

<p>So we moved all of logic out of the test scripts and put it into the fake services. The mantra \&ldquo;simple tests\ was printed out on a banner hung in the entrance to the development floor. Each fake would know where it was in and it would validate every message coming through. The tests became input data and nothing else.</p>

<p>Failures were decoupled from tests, too. We correlated which fakes were reporting which errors and then worked on getting the errors down to zero. Error handling went in to make the fakes as robust as possible.</p>

<p>The build light glowed green. And I started to question the value of having a test suite that was always green. You may be thinking &ldquo;isn&rsquo;t a green build enough?” but I wanted the team to be better. I wanted to go beyond green.</p>

<p>We were still finding bugs during manual testing. Just not as many. I realised our test coverage was too low. Our automation could only do so much. The manual testers were plugging gaps in our coverage but we didn&rsquo;t how the size of what we were covering. I knew there was one more source of test ideas that we could use: we cloned the traffic from production into our test. We were now testing live.</p>

<p>This was scandalous. We discovered whole flows through the system that were unknown to us. We had covered the common and high risk parts, but edge case combinations were numerous. Our fake services were not up to scratch. Each fake failure taught us something new. I was riding a wave, walking around the office high-fiving anyone and everyone. I felt confident we would do this.</p>

<p>The next week was the worst of my life. The deployment into production was a debacle. The systems we had built: incomprehensible. The messages they sent were gibberish and garbage. It didn&rsquo;t make any sense. We worked all day and night piecing together the wreckage. We didn&rsquo;t understand why production failed when the same traffic being copied into test, was working.</p>

<p>The problem we would learn that the new systems had never ever spoken to each other. We&rsquo;d used fake services in so many places that the first integration was production. There was no disaster recovery plan. That was built into the new system.</p>

<p>Production was still down, I heard we were losing money every day and we couldn&rsquo;t rollback.</p>

<p>A penny dropped. I read about a new technique called ‘Blue-Green deployment' where you cut-over from old to new by rerouting traffic. &ldquo;Why don&rsquo;t we cut-over the production to the test environment. We know test works”. It was crazy. We turned off the real mess and our ‘fake' services became production. The King is dead! Long live the King!</p>

<p>With that switch we had consolidated the platforms, upgrading all existing technology. We had a test suite, services that self validate their input, comprehensive application monitoring and a modern development culture. All that was missed was a few tiny edge cases of business functionality.</p>

<p>And that&rsquo;s what I pitched to the board.</p>

<p>&ldquo;It would be cheaper to tell our call centre people that we were no longer offering the missing services than to try and recover. I don&rsquo;t even know if we&rsquo;re making money on them.” I waited in the office for the board&rsquo;s verdict. They didn&rsquo;t like what I had done. But they would accept it if the team delivered the missing features before the end of the financial year. I would not be in the team. Or the company.</p>

<p>I heard later that their ESB wasn&rsquo;t even being used. They had sold it to one of their competitors and made up for the revenue lost during the production deployment.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Big Data Ain't So Big]]></title>
    <link href="http://thoughtworks.github.io/p2/issue01/big-data/"/>
    <updated>2013-06-03T00:00:00-04:00</updated>
    <id>http://thoughtworks.github.io/p2/issue01/big-data</id>
    <content type="html"><![CDATA[<blockquote><p>I went to a conference and heard &ldquo;We&rsquo;re making a significant shift towards more fact-based decision making. And I was kind of like, what do people usually use? Horoscopes?&rdquo; There&rsquo;s now a big drive to be more data-analytical.</p></blockquote>

<p>The first thing to know about big data according to Dave Coombes is that it&rsquo;s an overhyped term. &ldquo;There are few apocryphal stories like &lsquo;We had this huge amount of data and then we shuffled it and, you know, it told us the answer to life, the universe and everything&rsquo; but they are few and far between.” he says. "Generally, it&rsquo;s not a valid approach.&lsquo;”</p>

<p>Contrary to popular belief big data doesn&rsquo;t happen when you get to gigabytes or terabytes but refers to any data analysis that is beyond the existing capabilities of a particular business.</p>

<p>The problem of analytical demands outstripping capability is not new. &ldquo;We&rsquo;ve always been at the point where we don&rsquo;t quite have enough capacity.&rdquo;, Dave says, &ldquo;We&rsquo;re always overstretching.” He has a theory: that the traditional data requirements of transactional applications tend to grow linearly, but post-Internet boom companies such as Google or Facebook base their business on network effects. Those effects scale quadratically, meaning such companies were always going to outgrow their data handling capabilities.</p>

<p>Often, too much data is not the problem. Although some businesses need to do high volume analysis, such as understanding Twitter streams or access logs, it&rsquo;s not usually of their core business data. Most core business analysis is retrospective &ndash; describing things that happened last week, last month or the first quarter last year. But where things really get interesting is in predictive analysis – such as recommendation engines or unsupervised machine learning.</p>

<p>&ldquo;My background is in theoretical physics and so I still value the scientific method where you posit some hypothesis and then run an experiment to look at your data and work out if what you thought was is true, is actually true.”, Dave says. But when you need to move your business quickly, you need to test and evaluate these hypotheses quickly and that doesn&rsquo;t always leave room to build a traditional warehouse of historical data.</p>

<p>Dave&rsquo;s latest project takes the same lean approaches used in application development and applies them to the areas of analytics, reporting and business intelligence to tailor the advertisements web customers see.</p>

<p>A strategic approach would be to consider everything known about customers and coerce all the available data into a single form that meets that requirement. But designing a data strategy that works now and in the future is difficult. It&rsquo;s easier to take the problem we want to solve and work backwards. Dave asks, &ldquo;What is the next most important thing, that if the business knew the answer to, they&rsquo;d be able to make an informed decision?” When you start with a question, you can pull in the information you need rather than pushing everything into a model and analysing it.</p>

<p>Too often, says Dave, the Data Warehouse is forced to codify business rules about the data that other applications provide. The more effort put into creating the Data Warehouse, the more it becomes another application and, therefore, another database with it&rsquo;s own semantic model. The compounding downside is the data quality is heavily influenced by the quality of its sources.</p>

<p>Codifying rules in the Data Warehouse stops it being an aggregator and creates place where assumptions are concreted in. These assumptions mean that the further you get from the source of the data, the fuzzier the answers you get will be. Keeping the data raw means that storage can be decoupled from presentation allowing you to quickly create new, testable views of that data.</p>

<p>Even so, quickly prototyping new views of data isn&rsquo;t always the priority. With operational reporting such as end-of-year financial reporting, the accuracy of the information is valued above how quickly you can improve. With a single report such as an end-of-year financial report, you won&rsquo;t get a chance to improve.</p>

<p>But in predictive analysis any new information is an improvement. If you want to start by increasing your confidence that you&rsquo;re matching  an advertisement to a particular type of person by 10 or 20 percent, then you have a chance to iterate and get feedback on the impact of your analysis. It&rsquo;s a short cycle, and you can learn and adapt as you go.</p>

<p>The solution, Dave says, is not a custom reporting model. What is needed is to consider the analytics lifecycle when building applications. &ldquo;The term ad-hoc reporting gives me the shivers.” he says. "Analysis shouldn&rsquo;t be some sort of esoteric thing that sits off in a gridlock of Excel spreadsheets, or that last card in an Inception you write up when someone says ‘Oh, I guess we&rsquo;ll need reporting'. It should be something that drives your line of business.”</p>
]]></content>
  </entry>
  
</feed>
