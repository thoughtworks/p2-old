

<!doctype html>
<html lang="en">

<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"/>
<title>Magazine | Test Augmentation</title>

<link rel="shortcut icon" type="image/png" href="/p2/favicon.png" />
<link href="/p2/stylesheets/screen.css" rel="stylesheet" type="text/css" media="all"/>
<link rel="alternate" type="application/rss+xml" title="P2 Magazine" href="http://thoughtworks.github.io/p2/atom.xml"/>
<script src="http://modernizr.com/downloads/modernizr-2.5.3.js"></script>
<script src="/p2/assets/js/keymaster.min.js"></script>
</head>

<body class="wrapper">
<nav class="paginate">
  
    <a id="prev" href="/p2/issue03/showcase/" class="prev">&#9664;</a>
    <script type="text/javascript">
      key('left', function() {
        window.open($("#prev")[0].href, '_self', false);
      });
    </script>
  
  
    <a id="next" href="/p2/issue03/puzzle/" class="next">&#9654;</a>
    <script type="text/javascript">
      key('right', function(){
        window.open($("#next")[0].href, '_self', false);
      });
    </script>
  
</nav>

<div class="container">
    <section class="page">
    <nav class="issues">
    <ul class="clearfix">
        <li class="active"><a href="/p2/issue03/index/">Issue 03, August 2013</a></li>
        <li class="pull-right">Page 7 / 9 </li>
    </ul>
</nav>

    <article>
    <header>
        <h1>Test Augmentation</h1>
        <h3></h3>
        <p class="byline">
        Words and Music by Ryan Boucher
        </p>
    </header>
    <div class="article-body"><p>When a system has already been built and no automated tests have been written, automated testing is often brought in for one of three reasons: to improve quality, to reduce time spent testing or to reduce testing headcount. In such an environment I usually see two approaches to automation. The first is a breadth first approach: where we take a end-to-end, high value flows and automate them. The second option is depth first: where we take one system and wrap multiple tests around it before moving on. Any automation attempt is going to face challenges that get into the way of the delivering value in line with the original justification. I&rsquo;ll get onto these for later, as I want to talk about another option. An option often overlooked because it&rsquo;s not pure automation.</p>

<p>Instead of automating an entire test flow or even a system, aim to automate single, small steps. Something simple like &lsquo;create a basic order&rsquo; or &lsquo;submit an order&rsquo;. The testers must able to run these steps in any sequence. These automated steps will be disconnected from each other and can&rsquo;t be chained together without manual intervention. This disconnection and dependence on humans is this approach&rsquo;s strength.</p>

<p>To make it usable by humans; build a lightweight page for our disconnected steps. It could be something as simple as a page of Jenkins jobs, although some nice design wouldn&rsquo;t go astray. You may already know where I&rsquo;m heading, create three pages of actions: one full of fixtures (givens), one full of actions (whens) and a third with (thens).</p>

<p>The &lsquo;Given I have an order&rsquo; automation script, when run, goes off and performs its short task. When complete it displays a newly created Order ID. Jotting ID numbers down is something testers have been doing since a bug first crawled out of off-by-one gap. The tester can take that Order ID and plug it into the &lsquo;Then the order is complete&rsquo; step. That step reports a failure because the order isn&rsquo;t complete –it&rsquo;s brand new. But that&rsquo;s what the tester expected –so not a failed test. The tester makes a mental note and moves on. No automated suite goes green, red or is updated in any fashion.</p>

<p>So what? This is just interactive cucumber – reducing testers to being automation glue. I prefer to call them Cyborgs in Test. Part human, part machine, all tester. I jest, I call it test augmentation instead of test automation. The tester can chop and change between manual and automation as she sees fit. The value of the manual tester is their mind. Their ability to detect emergent scenarios. We lose that with automation and we consciously try and get it back with exploratory testing.</p>

<p style="text-align:center;"> ⁂</p>

<p>Previously, I mentioned that the all-or-nothing automation approach has to overcome a series of challenges –each more diabolical than the last –to deliver on it&rsquo;s original business justification. I&rsquo;m going to touch on each of these with the aim of showing how test augmentation can help.</p>

<p>The first of these is interoperability between different automation technologies. Each shift in system under test can require a different automation tool. Java in places, mainframe in others, OpenScript when we can&rsquo;t avoid it. AutoHotKey and Sikuli when all else fails. Interoperability between these technologies isn&rsquo;t trivial and adds a layer of brittleness that only interop knows. This brittleness can erode the teams trust in the test. One way to reduce the impact of interop is to not do it. Give humans the responsibility; they&rsquo;re much more flexible. With test augmentation the tester is managing the state between the various steps.</p>

<p>The longer your test flow the more susceptible you are to change. If you&rsquo;re putting tests around a system, you&rsquo;re wanting change. When a single step in the larger automated flow breaks, the entire flow is failed until that step works again. This failing test is a good thing, it&rsquo;s telling us that either our test is wrong or the system is wrong and we&rsquo;ll change one to make it work again. The downside is that until the test is fixed the entire flow is failing. This may mean that we&rsquo;re missing out on failures after this point. It&rsquo;s also likely that the testers will have to manually re-execute the entire flow –to be sure, before the step gets fixed. With test augmentation the tester can use automation up to the point that the system has changed, shift into 4x4 mode and exercise that new code as in depth as they feel necessary before making use of the automation again.</p>

<p>The next set of challenges are human. When automation is sold as a means to reduce head count. You&rsquo;ll get resistance from the testers to provide support. Why should they help you when you&rsquo;re trying to take their jobs? Test augmentation isn&rsquo;t a way to reduce headcount. It&rsquo;s about giving your testers a helping hand. It&rsquo;s about allowing them to spend more time thinking and less time doing menial repetitive tasks.</p>

<p>The final challenge that I&rsquo;ll cover is that automated tests are opaque. This results in the testers not being able to see and trust what the automated tests do. They are uncertain what gets checked, which paths through the system are taken, whether shortcuts are used, etc. The end result is the testers rerun the automated tests as manual tests. It&rsquo;s still the their neck on the line if the system breaks so they are disinclined to use the automation. By giving testers smaller chunks of automation, they can learn to trust each individual piece.</p>

<p>I recently worked at a client where we did some of this. There was a three day wait for a process to be completed. The testers sent a request to another team of testers who would action it and respond. We automated just that step to be completed in five minutes. That covered the 80% use case. It didn&rsquo;t handle every scenario, but it handled the most common one. Look for steps that get executed frequently or when executed, take a long time to complete.</p>

<p>The downside for this approach is that you don&rsquo;t have an automated suite that you run every night. I don&rsquo;t mind this. I see more value in getting the developers to start writing unit tests for the code their changing. This will run on every commit and when combined with test augmentation is going to give you faster test cycles and better quality.</p>
</div>
    <aside class="left-column clearfix">
   <div class="author">
      
      <div>
         <a href="#">
            <img class="author-avatar" src="/p2/images/ryan-avatar.jpg"/>
         </a>
         <div>Ryan Boucher</div>
         
         <a href="http://twitter.com/distributedlife">@distributedlife</a>
         
      </div>
      
   </div>
</aside>

    <footer>
    </footer>
</article>

</section>

</div>
<div id="bottom-pages">
    <nav class="paginate">
  
    <a id="prev" href="/p2/issue03/showcase/" class="prev">&#9664;</a>
    <script type="text/javascript">
      key('left', function() {
        window.open($("#prev")[0].href, '_self', false);
      });
    </script>
  
  
    <a id="next" href="/p2/issue03/puzzle/" class="next">&#9654;</a>
    <script type="text/javascript">
      key('right', function(){
        window.open($("#next")[0].href, '_self', false);
      });
    </script>
  
</nav>

</div>
</body>
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js"></script>
<script type="text/javascript">
if (typeof jQuery == 'undefined') {
    document.write(unescape("%3Cscript src='assets/js/jquery-1.8.2.min.js' type='text/javascript'%3E%3C/script%3E"));
}
</script>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-41328116-1', 'thoughtworks.github.io');
ga('send', 'pageview');
</script>
<script>
$(function() {
    setTimeout(function() {
        $("<span>&nbsp;</span>").appendTo(".cover");
    }, 100);
});
</script>
</html>
